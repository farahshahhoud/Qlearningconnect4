{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4InRow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1o-6sG28bE5U4bEEM1jldKrfXAHYUosZq",
      "authorship_tag": "ABX9TyO/GgFRuYhw7LzG3D1/vL2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farahshahhoud/Qlearningconnect4/blob/main/4InRow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sanVWWT94Jl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75819833-7fe1-4938-e5fc-bbd4f657a6f5"
      },
      "source": [
        "from abc import abstractmethod\n",
        "import os\n",
        "from pathlib import Path\n",
        "import keras.models as Km\n",
        "import keras as K\n",
        "import numpy as np\n",
        "import time\n",
        "import numpy as np\n",
        "import keras.layers as Kl\n",
        "from keras import optimizers\n",
        "from abc import abstractmethod\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JggK1SDp-G9-"
      },
      "source": [
        "class Model:\n",
        "\n",
        "    def __init__(self,tag):\n",
        "        self.tag = tag\n",
        "        self.epsilon = 0.1\n",
        "        self.alpha = 0.5\n",
        "        self.gamma = 1\n",
        "        self.model = self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.tag == 1:\n",
        "           tag = '_first'\n",
        "        else:\n",
        "            tag = '_second'''\n",
        "        s = 'model_values' + tag + '.h5'\n",
        "        model_file = Path(s)\n",
        "\n",
        "        if model_file.is_file():\n",
        "            print('load model:',s)\n",
        "            model = Km.load_model(s)\n",
        "        else:\n",
        "            model = self.create_model()\n",
        "        return model\n",
        "\n",
        "    @abstractmethod\n",
        "    def create_model(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def state_to_tensor(self, state, move):\n",
        "        pass\n",
        "\n",
        "    def calc_value(self, state, move):\n",
        "        tensor = self.state_to_tensor(state, move)\n",
        "        value = self.model.predict(tensor)\n",
        "        # K.backend.clear_session()\n",
        "        return value\n",
        "\n",
        "    def calc_target(self, prev_state, prev_move, state, ava_moves, reward):\n",
        "\n",
        "        v_s = self.calc_value(prev_state, prev_move)\n",
        "\n",
        "        v = []\n",
        "        for move in ava_moves:\n",
        "            v.append(self.calc_value(state, move))\n",
        "\n",
        "        if reward == 0:\n",
        "            if len(v) > 0:\n",
        "                v_s_tag = self.gamma * np.max(v)\n",
        "            else:\n",
        "                #print('no moves!!!')\n",
        "                v_s_tag = 0\n",
        "            target = np.array(v_s + self.alpha * (reward + v_s_tag - v_s))\n",
        "        else:\n",
        "            # v_s_tag = 0\n",
        "            target = reward\n",
        "\n",
        "        # target = np.array(v_s + self.alpha * (reward + v_s_tag - v_s))\n",
        "\n",
        "        # if self.tag == 1:\n",
        "        #     print('learn general')\n",
        "        #     print(prev_state, prev_move, state, ava_moves, reward)\n",
        "        #      print('target: ', target)\n",
        "\n",
        "        return target\n",
        "\n",
        "    def train_model(self, prev_state, prev_move, target, epochs):\n",
        "\n",
        "        tensor = self.state_to_tensor(prev_state, prev_move)\n",
        "\n",
        "        if target is not None:\n",
        "\n",
        "            self.model.fit(tensor, target, epochs=epochs, verbose=0)\n",
        "            K.backend.clear_session()\n",
        "\n",
        "    def save_model(self):\n",
        "        if self.tag == 1:\n",
        "            tag = '_first'\n",
        "        else:\n",
        "            tag = '_second'\n",
        "        s = 'model_values' + tag + '.h5'\n",
        "\n",
        "        try:\n",
        "            os.remove(s)\n",
        "        except:\n",
        "            pass\n",
        "        print('model save to: ',s)\n",
        "        self.model.save(s)\n",
        "\n",
        "    def learn_batch(self, memory):\n",
        "        print('start learning player', self.tag)\n",
        "        print('data length:', len(memory))\n",
        "\n",
        "        # build x_train\n",
        "        ind = 0\n",
        "        x_train = np.zeros((len(memory), 7, 7, 1))\n",
        "        # x_train = np.zeros((len(memory), 2, 9))\n",
        "        for v in memory:\n",
        "            [prev_state, prev_move, _, _, _] = v\n",
        "            sample = self.state_to_tensor(prev_state, prev_move)\n",
        "            x_train[ind, :, :, :] = sample\n",
        "            ind += 1\n",
        "\n",
        "        # train with planning\n",
        "        # for i in range(5):\n",
        "        loss = 20\n",
        "        count = 0\n",
        "        while loss > 0.02 and count<20:\n",
        "            # tic()\n",
        "            y_train = self.create_targets(memory)\n",
        "            # toc()\n",
        "            self.model.fit(x_train, y_train, epochs=5, batch_size=256, verbose=1)\n",
        "            loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)[0]\n",
        "            count += 1\n",
        "            #print('planning number:', count, 'loss', loss)\n",
        "\n",
        "        loss = self.model.evaluate(x_train, y_train, batch_size=256, verbose=0)\n",
        "        print('player:', self.tag,'loss: ', loss, 'loops: ', count)\n",
        "\n",
        "        self.save_model()\n",
        "\n",
        "    def create_targets(self, memory):\n",
        "        y_train_ = np.zeros((len(memory), 1))\n",
        "        count_ = 0\n",
        "        for v_ in memory:\n",
        "            [prev_state_, prev_move_, state_, ava_moves_, reward_] = v_\n",
        "            target = self.calc_target(prev_state_, prev_move_, state_, ava_moves_, reward_)\n",
        "            y_train_[count_, :] = target\n",
        "            count_ += 1\n",
        "            #print('---------')\n",
        "            #print('player', self.tag)\n",
        "            #print('prev state', prev_state_)\n",
        "            #print('prev move', prev_move_)\n",
        "            #print('state', state_)\n",
        "            #print('ava moves', ava_moves_)\n",
        "            #print('reward', reward_)\n",
        "            #print('target', target)\n",
        "            \n",
        "            #value = self.calc_value(prev_state_, prev_move_)\n",
        "            #print('value through net', value)\n",
        "            #time.sleep(0.2)\n",
        "\n",
        "        return y_train_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPiI_VaIFFXU"
      },
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, tag,exploration_factor=1):\n",
        "        self.tag = tag\n",
        "        self.exp_factor = exploration_factor\n",
        "        self.prev_state = np.zeros((6, 7))\n",
        "        self.prev_move = -1\n",
        "        self.state = None\n",
        "        self.move = None\n",
        "        self.print_value = False\n",
        "        self.model = Model(self.tag)\n",
        "        self.memory = []\n",
        "        self.count_memory = 0\n",
        "        self.winner_flag = False\n",
        "\n",
        "    def choose_move(self, state, winner, learn):\n",
        "\n",
        "        self.load_to_memory(self.prev_state, self.prev_move, state, self.ava_moves(state), self.reward(winner))\n",
        "\n",
        "        if winner is not None:\n",
        "\n",
        "            self.count_memory += 1\n",
        "\n",
        "            self.prev_state = np.zeros((6, 7))\n",
        "            self.prev_move = -1\n",
        "\n",
        "            if learn is True and self.count_memory == 1000:\n",
        "                self.count_memory = 0\n",
        "                # Offline training\n",
        "                self.model.learn_batch(self.memory)\n",
        "                self.memory = []\n",
        "                # Online training\n",
        "                #self.learn(self.prev_state, self.prev_move, state, self.ava_moves(state),  -1, self.reward(winner))\n",
        "            return None\n",
        "\n",
        "        p = random.uniform(0, 1)\n",
        "\n",
        "        if p < self.exp_factor:\n",
        "            idx = self.choose_optimal_move(state)\n",
        "        else:\n",
        "            ava_moves = self.ava_moves(state)\n",
        "            idx = random.choice(ava_moves)\n",
        "\n",
        "        self.prev_state = state\n",
        "        self.prev_move = idx\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def choose_optimal_move(self, state):\n",
        "\n",
        "        ava_moves = self.ava_moves(state)\n",
        "        v = -float('Inf')\n",
        "        v_list = []\n",
        "        idx = []\n",
        "        for move in ava_moves:\n",
        "            value = self.model.calc_value(state, move)\n",
        "            v_list.append(round(float(value), 5))\n",
        "\n",
        "            if value > v:\n",
        "                v = value\n",
        "                idx = [move]\n",
        "            elif v == value:\n",
        "                idx.append(move)\n",
        "\n",
        "        idx = random.choice(idx)\n",
        "        return idx\n",
        "\n",
        "    def game_winner(self, state):\n",
        "        winner = None\n",
        "        for i in range(len(state[:,0])-3):\n",
        "            for j in range(len(state[0, :])-3):\n",
        "                winner = self.square_winner(state[i:i+4, j:j+4])\n",
        "                if winner is not None:\n",
        "                    # print('winner is:', self.winner)\n",
        "                    break\n",
        "            if winner is not None:\n",
        "                # print('winner is:', self.winner)\n",
        "                break\n",
        "\n",
        "        if np.min(np.abs(state[0, :])) != 0:\n",
        "            winner = 0\n",
        "            # print('no winner')\n",
        "\n",
        "        return winner\n",
        "\n",
        "    @staticmethod\n",
        "    def square_winner(square):\n",
        "        s = np.append([np.sum(square, axis=0), np.sum(square, axis=1).T],\n",
        "                      [np.trace(square), np.flip(square,axis=1).trace()])\n",
        "        if np.max(s) == 4:\n",
        "            winner = 1\n",
        "        elif np.min(s) == -4:\n",
        "            winner = 2\n",
        "        else:\n",
        "            winner = None\n",
        "        return winner\n",
        "\n",
        "    @staticmethod\n",
        "    def make_state_from_move(state, move, player):\n",
        "        if move is None:\n",
        "            return state\n",
        "\n",
        "        state = np.array(state)\n",
        "        if player == 1:\n",
        "            tag = 1\n",
        "        else:\n",
        "            tag = -1\n",
        "\n",
        "        if len(np.where(state[:, move] == 0)[0]) == 0:\n",
        "            print(state)\n",
        "        idy = np.where(state[:, move] == 0)[0][-1]\n",
        "        state = np.array(state)\n",
        "        state[idy, move] = tag\n",
        "\n",
        "        return state\n",
        "\n",
        "    def reward(self, winner):\n",
        "\n",
        "        if winner is self.tag:\n",
        "            reward = 1\n",
        "        elif winner is None:\n",
        "            reward = 0\n",
        "        elif winner == 0:\n",
        "            reward = 0.5\n",
        "        else:\n",
        "            reward = -1\n",
        "        return reward\n",
        "\n",
        "    def learn(self, prev_state, prev_move, state, ava_moves, move, reward):\n",
        "\n",
        "        if prev_move != -1:\n",
        "\n",
        "            target = self.model.calc_target(prev_state, prev_move, state, ava_moves, reward)\n",
        "            # print(target)\n",
        "            self.model.train_model(prev_state, prev_move, target, 1)\n",
        "\n",
        "    @abstractmethod\n",
        "    def ava_moves(self, state):\n",
        "        pass\n",
        "\n",
        "    def load_to_memory(self, prev_state, prev_move, state, ava_moves, reward):\n",
        "        self.memory.append([prev_state, prev_move, state, ava_moves, reward])\n",
        "\n",
        "    def save_memory(self):\n",
        "        is_file_ = True\n",
        "        count = 1\n",
        "        s = ''\n",
        "        while is_file_:\n",
        "            s = 'value_list_' + str(self.tag) + '_' + str(count) + '.pkl'\n",
        "            if Path(s).is_file():\n",
        "                is_file_ = True\n",
        "                count = count + 1\n",
        "            else:\n",
        "                is_file_ = False\n",
        "\n",
        "        with open(s, 'wb') as output:\n",
        "            pickle.dump(self.memory, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Pqx3UrFGjM"
      },
      "source": [
        "class Game:\n",
        "\n",
        "    def __init__(self, player1, player2, exp1=1, exp2=1, tag1=1, tag2=2):\n",
        "\n",
        "        self.players = {1: player1(tag1, exploration_factor=exp1),\n",
        "                        2: player2(tag2, exploration_factor=exp2)}\n",
        "\n",
        "        self.state, self.winner, self.turn = self.init_game()\n",
        "        self.memory = {}\n",
        "\n",
        "    def play_game(self, learn=False):\n",
        "\n",
        "        move_count = 0\n",
        "\n",
        "        while self.winner is None:\n",
        "            move = self.play_move(learn)\n",
        "\n",
        "            self.state = self.make_state_from_move(move)\n",
        "            self.game_winner()\n",
        "            if self.winner is not None:\n",
        "              break\n",
        "            self.next_player()\n",
        "            move_count += 1\n",
        "\n",
        "        self.play_move(learn)  \n",
        "        self.next_player()      \n",
        "        self.play_move(learn)   \n",
        "        self.next_player()    \n",
        "\n",
        "        return self.winner, move_count\n",
        "\n",
        "    def play_move(self, learn):\n",
        "        player = self.players[self.turn]\n",
        "        move=1\n",
        "        if self.winner is None:\n",
        "          move = player.choose_move(self.state, self.winner, learn)\n",
        "        return move\n",
        "\n",
        "    def play_multiple_games(self, episodes, learn):\n",
        "        statistics = {1: 0, 2: 0, 0: 0, 'move_count': 0}\n",
        "        move_count_total = []\n",
        "        for i in range(episodes):\n",
        "            winner, move_count = self.play_game(learn)\n",
        "            move_count_total.append(move_count)\n",
        "            statistics[winner] = statistics[winner] + 1\n",
        "\n",
        "            self.state, self.winner, self.turn = self.init_game()\n",
        "\n",
        "        statistics['move_count'] = np.mean(move_count_total)\n",
        "        return statistics\n",
        "\n",
        "    @abstractmethod\n",
        "    def init_game(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def make_state_from_move(self, move):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def next_player(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def game_winner(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def print_game(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxnMygphEHRy"
      },
      "source": [
        "class ConnectFourModel(Model):\n",
        "\n",
        "    def __init__(self, tag):\n",
        "        super().__init__(tag)\n",
        "        pass\n",
        "\n",
        "    def create_model(self):\n",
        "        print('new model')\n",
        "\n",
        "        model = Km.Sequential()\n",
        "        model.add(Kl.Conv2D(20, (5, 5), padding='same', input_shape=(7, 7, 1)))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "        model.add(Kl.Conv2D(20, (4, 4), padding='same'))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "        model.add(Kl.Conv2D(20, (4, 4), padding='same'))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "        model.add(Kl.Conv2D(30, (4, 4), padding='same'))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "        model.add(Kl.Conv2D(30, (4, 4), padding='same'))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "        model.add(Kl.Conv2D(30, (4, 4), padding='same'))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "        model.add(Kl.Conv2D(30, (4, 4), padding='same'))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "\n",
        "        model.add(Kl.Flatten(input_shape=(7, 7, 1)))\n",
        "        model.add(Kl.Dense(49))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "        model.add(Kl.Dense(7))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "        model.add(Kl.Dense(7))\n",
        "        model.add(Kl.LeakyReLU(alpha=0.3))\n",
        "\n",
        "        model.add(Kl.Dense(1, activation='linear'))\n",
        "        opt = optimizers.adam()\n",
        "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def state_to_tensor(self, state, move):\n",
        "\n",
        "        vec = np.zeros((1, 7))\n",
        "        if move != -1:\n",
        "            vec[0, move] = 1\n",
        "        tensor = np.append(vec, state, axis=0)\n",
        "        tensor = tensor.reshape((1, 7, 7, 1))\n",
        "\n",
        "        return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PPx_TgCEyKC"
      },
      "source": [
        "class ConnectFour(Game):\n",
        "\n",
        "    def __init__(self, player1, player2, exp1=1, exp2=1, tag1=1, tag2=2):\n",
        "        super().__init__(player1, player2, exp1, exp2, tag1, tag2)\n",
        "\n",
        "    def init_game(self):\n",
        "        return np.zeros((6, 7)), None, 1\n",
        "\n",
        "    def make_state_from_move(self, move):\n",
        "        if move is None:\n",
        "            return self.state\n",
        "\n",
        "        state = np.array(self.state)\n",
        "        if self.turn == 1:\n",
        "            tag = 1\n",
        "        else:\n",
        "            tag = -1\n",
        "\n",
        "        if len(np.where(state[:, move] == 0)[0]) == 0:\n",
        "            print(state)\n",
        "        idy = np.where(state[:, move] == 0)[0][-1]\n",
        "        state = np.array(state)\n",
        "        state[idy, move] = tag\n",
        "\n",
        "        return state\n",
        "\n",
        "    def next_player(self):\n",
        "        if self.turn == 1:\n",
        "            self.turn = 2\n",
        "        else:\n",
        "            self.turn = 1\n",
        "\n",
        "    def game_winner(self):\n",
        "        for i in range(len(self.state[:,0])-3):\n",
        "            for j in range(len(self.state[0, :])-3):\n",
        "                self.square_winner(self.state[i:i+4, j:j+4])\n",
        "                if self.winner is not None:\n",
        "                    # print('winner is:', self.winner)\n",
        "                    break\n",
        "            if self.winner is not None:\n",
        "                print('winner is:', self.winner)\n",
        "                break\n",
        "\n",
        "        if np.min(np.abs(self.state[0, :])) != 0:\n",
        "            self.winner = 0\n",
        "            print('no winner')\n",
        "\n",
        "    def square_winner(self, square):\n",
        "        s = np.append([np.sum(square, axis=0), np.sum(square, axis=1).T],\n",
        "                      [np.trace(square), np.flip(square,axis=1).trace()])\n",
        "        if np.max(s) == 4:\n",
        "            self.winner = 1\n",
        "        elif np.min(s) == -4:\n",
        "            self.winner = 2\n",
        "        else:\n",
        "            self.winner = None\n",
        "        return self.winner\n",
        "\n",
        "    def print_game(self):\n",
        "\n",
        "      print('  --------------')\n",
        "      charar = np.chararray(state.shape,unicode=True)\n",
        "      for i in range(state.shape[0]):\n",
        "        for j in range(state.shape[1]):\n",
        "          if state[i][j]==1: charar[i][j]='*'\n",
        "          elif state[i][j]==-1: charar[i][j]='-'\n",
        "          else: charar[i][j]='0'\n",
        "      print(charar)\n",
        "      print('  --------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JvWkd1JEmTI"
      },
      "source": [
        "class HumanPlayer:\n",
        "\n",
        "    def __init__(self, tag, exploration_factor=1):\n",
        "        self.print_value = False\n",
        "        self.exp_factor = exploration_factor\n",
        "        self.tag = tag\n",
        "\n",
        "    @staticmethod\n",
        "    def choose_move(state, winner, learn):\n",
        "        if winner is not None:\n",
        "          return 1\n",
        "        print('  --------------')\n",
        "        charar = np.chararray(state.shape,unicode=True)\n",
        "        for i in range(state.shape[0]):\n",
        "          for j in range(state.shape[1]):\n",
        "            if state[i][j]==1: charar[i][j]='*'\n",
        "            elif state[i][j]==-1: charar[i][j]='-'\n",
        "            else: charar[i][j]='0'\n",
        "        print(charar)\n",
        "        print('  --------------')\n",
        "        idx = int(input('Choose move number: ')) - 1\n",
        "        return idx\n",
        "\n",
        "    def save_memory(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkvG1dQ_EsQH"
      },
      "source": [
        "class ConnectFourAgent(Agent):\n",
        "\n",
        "    def __init__(self, tag, exploration_factor=1):\n",
        "\n",
        "        super().__init__(tag, exploration_factor)\n",
        "        self.model = ConnectFourModel(tag)\n",
        "        self.prev_state = np.zeros((6, 7))\n",
        "\n",
        "    def ava_moves(self, state):\n",
        "        moves = np.where(state[0, :] == 0)[0]\n",
        "        return moves"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weGih4AvEOIa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed989923-71c0-44b7-b379-3d464ae040e3"
      },
      "source": [
        "episodes = 6000\n",
        "\n",
        "game = ConnectFour(ConnectFourAgent, ConnectFourAgent, 0.7, 0.7)\n",
        "statistics = game.play_multiple_games(episodes, learn=True)\n",
        "print('0.7, 0.7', statistics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new model\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 7, 7, 20)          520       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 20)          6420      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 20)          6420      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 7, 7, 30)          9630      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 30)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 30)          14430     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 7, 7, 30)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 30)          14430     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 30)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 7, 7, 30)          14430     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 7, 7, 30)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1470)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 49)                72079     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 49)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 350       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 7)                 0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7)                 56        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 7)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 8         \n",
            "=================================================================\n",
            "Total params: 138,773\n",
            "Trainable params: 138,773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "new model\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 7, 7, 20)          520       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 7, 7, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 7, 7, 20)          6420      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 7, 7, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 7, 7, 20)          6420      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 7, 7, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 7, 7, 30)          9630      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 7, 7, 30)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 7, 7, 30)          14430     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 7, 7, 30)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 7, 7, 30)          14430     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 7, 7, 30)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 7, 7, 30)          14430     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 7, 7, 30)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1470)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 49)                72079     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 49)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 7)                 350       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 7)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 7)                 56        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)   (None, 7)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 8         \n",
            "=================================================================\n",
            "Total params: 138,773\n",
            "Trainable params: 138,773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "start learning player 1\n",
            "data length: 11174\n",
            "Epoch 1/5\n",
            "11174/11174 [==============================] - 9s 803us/step - loss: 0.0857 - accuracy: 3.5797e-04\n",
            "Epoch 2/5\n",
            "11174/11174 [==============================] - 9s 812us/step - loss: 0.0772 - accuracy: 0.0095\n",
            "Epoch 3/5\n",
            "11174/11174 [==============================] - 9s 785us/step - loss: 0.0620 - accuracy: 0.0236\n",
            "Epoch 4/5\n",
            "11174/11174 [==============================] - 9s 761us/step - loss: 0.0510 - accuracy: 0.0357\n",
            "Epoch 5/5\n",
            "11174/11174 [==============================] - 8s 742us/step - loss: 0.0444 - accuracy: 0.0425\n",
            "Epoch 1/5\n",
            "11174/11174 [==============================] - 9s 844us/step - loss: 0.0628 - accuracy: 0.0391\n",
            "Epoch 2/5\n",
            "11174/11174 [==============================] - 9s 840us/step - loss: 0.0458 - accuracy: 0.0495\n",
            "Epoch 3/5\n",
            "11174/11174 [==============================] - 9s 796us/step - loss: 0.0396 - accuracy: 0.0525\n",
            "Epoch 4/5\n",
            "11174/11174 [==============================] - 9s 798us/step - loss: 0.0361 - accuracy: 0.0560\n",
            "Epoch 5/5\n",
            "11174/11174 [==============================] - 9s 794us/step - loss: 0.0357 - accuracy: 0.0564\n",
            "Epoch 1/5\n",
            "11174/11174 [==============================] - 9s 789us/step - loss: 0.0379 - accuracy: 0.0647\n",
            "Epoch 2/5\n",
            "11174/11174 [==============================] - 9s 797us/step - loss: 0.0326 - accuracy: 0.0672\n",
            "Epoch 3/5\n",
            "11174/11174 [==============================] - 9s 795us/step - loss: 0.0297 - accuracy: 0.0690\n",
            "Epoch 4/5\n",
            "11174/11174 [==============================] - 9s 796us/step - loss: 0.0265 - accuracy: 0.0711\n",
            "Epoch 5/5\n",
            "11174/11174 [==============================] - 9s 812us/step - loss: 0.0231 - accuracy: 0.0735\n",
            "Epoch 1/5\n",
            "11174/11174 [==============================] - 9s 821us/step - loss: 0.0318 - accuracy: 0.0730\n",
            "Epoch 2/5\n",
            "11174/11174 [==============================] - 9s 808us/step - loss: 0.0261 - accuracy: 0.0755\n",
            "Epoch 3/5\n",
            "11174/11174 [==============================] - 9s 816us/step - loss: 0.0219 - accuracy: 0.0793\n",
            "Epoch 4/5\n",
            "11174/11174 [==============================] - 9s 833us/step - loss: 0.0201 - accuracy: 0.0800\n",
            "Epoch 5/5\n",
            "11174/11174 [==============================] - 9s 808us/step - loss: 0.0201 - accuracy: 0.0801\n",
            "player: 1 loss:  [0.015360018643493066, 0.08304993808269501] loops:  4\n",
            "model save to:  model_values_first.h5\n",
            "start learning player 2\n",
            "data length: 10594\n",
            "Epoch 1/5\n",
            "10594/10594 [==============================] - 9s 837us/step - loss: 0.0912 - accuracy: 1.8879e-04\n",
            "Epoch 2/5\n",
            "10594/10594 [==============================] - 8s 796us/step - loss: 0.0849 - accuracy: 0.0052\n",
            "Epoch 3/5\n",
            "10594/10594 [==============================] - 8s 773us/step - loss: 0.0825 - accuracy: 0.0089\n",
            "Epoch 4/5\n",
            "10594/10594 [==============================] - 8s 791us/step - loss: 0.0768 - accuracy: 0.0163\n",
            "Epoch 5/5\n",
            "10594/10594 [==============================] - 8s 797us/step - loss: 0.0736 - accuracy: 0.0197\n",
            "Epoch 1/5\n",
            "10594/10594 [==============================] - 9s 871us/step - loss: 0.0645 - accuracy: 0.0255\n",
            "Epoch 2/5\n",
            "10594/10594 [==============================] - 9s 835us/step - loss: 0.0589 - accuracy: 0.0327\n",
            "Epoch 3/5\n",
            "10594/10594 [==============================] - 9s 820us/step - loss: 0.0524 - accuracy: 0.0393\n",
            "Epoch 4/5\n",
            "10594/10594 [==============================] - 9s 849us/step - loss: 0.0508 - accuracy: 0.0414\n",
            "Epoch 5/5\n",
            "10594/10594 [==============================] - 9s 836us/step - loss: 0.0464 - accuracy: 0.0446\n",
            "Epoch 1/5\n",
            "10594/10594 [==============================] - 9s 832us/step - loss: 0.0381 - accuracy: 0.0576\n",
            "Epoch 2/5\n",
            "10594/10594 [==============================] - 9s 813us/step - loss: 0.0364 - accuracy: 0.0604\n",
            "Epoch 3/5\n",
            "10594/10594 [==============================] - 9s 804us/step - loss: 0.0328 - accuracy: 0.0627\n",
            "Epoch 4/5\n",
            "10594/10594 [==============================] - 9s 811us/step - loss: 0.0289 - accuracy: 0.0701\n",
            "Epoch 5/5\n",
            "10594/10594 [==============================] - 9s 822us/step - loss: 0.0254 - accuracy: 0.0731\n",
            "Epoch 1/5\n",
            "10594/10594 [==============================] - 9s 803us/step - loss: 0.0321 - accuracy: 0.0752\n",
            "Epoch 2/5\n",
            "10594/10594 [==============================] - 9s 803us/step - loss: 0.0306 - accuracy: 0.0749\n",
            "Epoch 3/5\n",
            "10594/10594 [==============================] - 8s 788us/step - loss: 0.0239 - accuracy: 0.0804\n",
            "Epoch 4/5\n",
            "10594/10594 [==============================] - 9s 815us/step - loss: 0.0223 - accuracy: 0.0822\n",
            "Epoch 5/5\n",
            "10594/10594 [==============================] - 9s 813us/step - loss: 0.0207 - accuracy: 0.0830\n",
            "player: 2 loss:  [0.01945895760107208, 0.08495374768972397] loops:  4\n",
            "model save to:  model_values_second.h5\n",
            "start learning player 1\n",
            "data length: 7424\n",
            "Epoch 1/5\n",
            "7424/7424 [==============================] - 6s 788us/step - loss: 0.0831 - accuracy: 0.0870\n",
            "Epoch 2/5\n",
            "7424/7424 [==============================] - 6s 788us/step - loss: 0.0629 - accuracy: 0.1024\n",
            "Epoch 3/5\n",
            "7424/7424 [==============================] - 6s 779us/step - loss: 0.0542 - accuracy: 0.1080\n",
            "Epoch 4/5\n",
            "7424/7424 [==============================] - 6s 774us/step - loss: 0.0443 - accuracy: 0.1125\n",
            "Epoch 5/5\n",
            "7424/7424 [==============================] - 6s 777us/step - loss: 0.0406 - accuracy: 0.1162\n",
            "Epoch 1/5\n",
            "7424/7424 [==============================] - 6s 797us/step - loss: 0.0364 - accuracy: 0.1208\n",
            "Epoch 2/5\n",
            "7424/7424 [==============================] - 6s 802us/step - loss: 0.0293 - accuracy: 0.1247\n",
            "Epoch 3/5\n",
            "7424/7424 [==============================] - 6s 797us/step - loss: 0.0257 - accuracy: 0.1261\n",
            "Epoch 4/5\n",
            "7424/7424 [==============================] - 6s 791us/step - loss: 0.0240 - accuracy: 0.1272\n",
            "Epoch 5/5\n",
            "7424/7424 [==============================] - 6s 793us/step - loss: 0.0205 - accuracy: 0.1289\n",
            "player: 1 loss:  [0.018161736492966783, 0.13065733015537262] loops:  2\n",
            "model save to:  model_values_first.h5\n",
            "start learning player 2\n",
            "data length: 6787\n",
            "Epoch 1/5\n",
            "6787/6787 [==============================] - 5s 792us/step - loss: 0.1291 - accuracy: 0.0748\n",
            "Epoch 2/5\n",
            "6787/6787 [==============================] - 5s 798us/step - loss: 0.0984 - accuracy: 0.0936\n",
            "Epoch 3/5\n",
            "6787/6787 [==============================] - 5s 797us/step - loss: 0.0874 - accuracy: 0.0987\n",
            "Epoch 4/5\n",
            "6787/6787 [==============================] - 5s 801us/step - loss: 0.0750 - accuracy: 0.1120\n",
            "Epoch 5/5\n",
            "6787/6787 [==============================] - 6s 813us/step - loss: 0.0760 - accuracy: 0.1089\n",
            "Epoch 1/5\n",
            "6787/6787 [==============================] - 10s 2ms/step - loss: 0.0508 - accuracy: 0.1238\n",
            "Epoch 2/5\n",
            "6787/6787 [==============================] - 6s 942us/step - loss: 0.0396 - accuracy: 0.1310\n",
            "Epoch 3/5\n",
            "6787/6787 [==============================] - 6s 818us/step - loss: 0.0389 - accuracy: 0.1311\n",
            "Epoch 4/5\n",
            "6787/6787 [==============================] - 6s 812us/step - loss: 0.0323 - accuracy: 0.1354\n",
            "Epoch 5/5\n",
            "6787/6787 [==============================] - 5s 794us/step - loss: 0.0284 - accuracy: 0.1379\n",
            "Epoch 1/5\n",
            "6787/6787 [==============================] - 5s 808us/step - loss: 0.0343 - accuracy: 0.1360\n",
            "Epoch 2/5\n",
            "6787/6787 [==============================] - 5s 796us/step - loss: 0.0325 - accuracy: 0.1382\n",
            "Epoch 3/5\n",
            "6787/6787 [==============================] - 5s 794us/step - loss: 0.0265 - accuracy: 0.1388\n",
            "Epoch 4/5\n",
            "6787/6787 [==============================] - 5s 790us/step - loss: 0.0260 - accuracy: 0.1386\n",
            "Epoch 5/5\n",
            "6787/6787 [==============================] - 5s 791us/step - loss: 0.0242 - accuracy: 0.1401\n",
            "Epoch 1/5\n",
            "6787/6787 [==============================] - 5s 792us/step - loss: 0.0253 - accuracy: 0.1403\n",
            "Epoch 2/5\n",
            "6787/6787 [==============================] - 5s 790us/step - loss: 0.0227 - accuracy: 0.1416\n",
            "Epoch 3/5\n",
            "6787/6787 [==============================] - 5s 791us/step - loss: 0.0214 - accuracy: 0.1420\n",
            "Epoch 4/5\n",
            "6787/6787 [==============================] - 5s 789us/step - loss: 0.0235 - accuracy: 0.1413\n",
            "Epoch 5/5\n",
            "6787/6787 [==============================] - 5s 774us/step - loss: 0.0223 - accuracy: 0.1420\n",
            "player: 2 loss:  [0.018737379120119687, 0.1435096561908722] loops:  4\n",
            "model save to:  model_values_second.h5\n",
            "start learning player 1\n",
            "data length: 9959\n",
            "Epoch 1/5\n",
            "9959/9959 [==============================] - 8s 794us/step - loss: 0.1000 - accuracy: 0.0521\n",
            "Epoch 2/5\n",
            "9959/9959 [==============================] - 8s 782us/step - loss: 0.0802 - accuracy: 0.0611\n",
            "Epoch 3/5\n",
            "9959/9959 [==============================] - 8s 796us/step - loss: 0.0706 - accuracy: 0.0650\n",
            "Epoch 4/5\n",
            "9959/9959 [==============================] - 8s 771us/step - loss: 0.0622 - accuracy: 0.0691\n",
            "Epoch 5/5\n",
            "9959/9959 [==============================] - 8s 765us/step - loss: 0.0576 - accuracy: 0.0725\n",
            "Epoch 1/5\n",
            "9959/9959 [==============================] - 8s 768us/step - loss: 0.0469 - accuracy: 0.0770\n",
            "Epoch 2/5\n",
            "9959/9959 [==============================] - 8s 757us/step - loss: 0.0410 - accuracy: 0.0789\n",
            "Epoch 3/5\n",
            "9959/9959 [==============================] - 7s 751us/step - loss: 0.0353 - accuracy: 0.0836\n",
            "Epoch 4/5\n",
            "9959/9959 [==============================] - 8s 771us/step - loss: 0.0320 - accuracy: 0.0851\n",
            "Epoch 5/5\n",
            "9959/9959 [==============================] - 8s 779us/step - loss: 0.0287 - accuracy: 0.0872\n",
            "Epoch 1/5\n",
            "9959/9959 [==============================] - 8s 764us/step - loss: 0.0271 - accuracy: 0.0875\n",
            "Epoch 2/5\n",
            "9959/9959 [==============================] - 7s 744us/step - loss: 0.0232 - accuracy: 0.0903\n",
            "Epoch 3/5\n",
            "9959/9959 [==============================] - 8s 755us/step - loss: 0.0212 - accuracy: 0.0917\n",
            "Epoch 4/5\n",
            "9959/9959 [==============================] - 8s 784us/step - loss: 0.0187 - accuracy: 0.0934\n",
            "Epoch 5/5\n",
            "9959/9959 [==============================] - 8s 780us/step - loss: 0.0169 - accuracy: 0.0945\n",
            "player: 1 loss:  [0.014548209212489971, 0.094386987388134] loops:  3\n",
            "model save to:  model_values_first.h5\n",
            "start learning player 2\n",
            "data length: 9425\n",
            "Epoch 1/5\n",
            "9425/9425 [==============================] - 7s 779us/step - loss: 0.1212 - accuracy: 0.0540\n",
            "Epoch 2/5\n",
            "9425/9425 [==============================] - 8s 833us/step - loss: 0.0931 - accuracy: 0.0677\n",
            "Epoch 3/5\n",
            "9425/9425 [==============================] - 7s 787us/step - loss: 0.0801 - accuracy: 0.0685\n",
            "Epoch 4/5\n",
            "9425/9425 [==============================] - 7s 782us/step - loss: 0.0676 - accuracy: 0.0776\n",
            "Epoch 5/5\n",
            "9425/9425 [==============================] - 7s 779us/step - loss: 0.0619 - accuracy: 0.0805\n",
            "Epoch 1/5\n",
            "9425/9425 [==============================] - 7s 772us/step - loss: 0.0474 - accuracy: 0.0839\n",
            "Epoch 2/5\n",
            "9425/9425 [==============================] - 7s 763us/step - loss: 0.0393 - accuracy: 0.0886\n",
            "Epoch 3/5\n",
            "9425/9425 [==============================] - 7s 793us/step - loss: 0.0340 - accuracy: 0.0914\n",
            "Epoch 4/5\n",
            "9425/9425 [==============================] - 7s 787us/step - loss: 0.0318 - accuracy: 0.0929\n",
            "Epoch 5/5\n",
            "9425/9425 [==============================] - 7s 772us/step - loss: 0.0276 - accuracy: 0.0956\n",
            "Epoch 1/5\n",
            "9425/9425 [==============================] - 7s 788us/step - loss: 0.0271 - accuracy: 0.0959\n",
            "Epoch 2/5\n",
            "9425/9425 [==============================] - 7s 766us/step - loss: 0.0250 - accuracy: 0.0977\n",
            "Epoch 3/5\n",
            "9425/9425 [==============================] - 7s 782us/step - loss: 0.0243 - accuracy: 0.0988\n",
            "Epoch 4/5\n",
            "9425/9425 [==============================] - 7s 774us/step - loss: 0.0218 - accuracy: 0.0989\n",
            "Epoch 5/5\n",
            "9425/9425 [==============================] - 7s 775us/step - loss: 0.0188 - accuracy: 0.1007\n",
            "player: 2 loss:  [0.019461140522056455, 0.1019628643989563] loops:  3\n",
            "model save to:  model_values_second.h5\n",
            "0.7, 0.7 {1: 1751, 2: 1248, 0: 1, 'move_count': 16.454333333333334}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfPrV1bBbCXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a544827b-2aa9-4c14-d809-f8dcb5a7634a"
      },
      "source": [
        "episodes = 1\n",
        "\n",
        "game = ConnectFour( ConnectFourAgent,HumanPlayer, 1,1)\n",
        "statistics = game.play_multiple_games(episodes, learn=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load model: model_values_first.h5\n",
            "load model: model_values_first.h5\n",
            "  --------------\n",
            "[['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '*' '0' '0' '0']]\n",
            "  --------------\n",
            "Choose move number: 3\n",
            "  --------------\n",
            "[['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '*' '0' '0' '0']\n",
            " ['0' '0' '-' '*' '0' '0' '0']]\n",
            "  --------------\n",
            "Choose move number: 2\n",
            "  --------------\n",
            "[['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '*' '0' '0' '0']\n",
            " ['0' '0' '0' '*' '0' '0' '0']\n",
            " ['0' '-' '-' '*' '0' '0' '0']]\n",
            "  --------------\n",
            "Choose move number: 4\n",
            "  --------------\n",
            "[['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '-' '0' '0' '0']\n",
            " ['0' '0' '0' '*' '0' '0' '0']\n",
            " ['0' '0' '0' '*' '0' '0' '0']\n",
            " ['0' '-' '-' '*' '*' '0' '0']]\n",
            "  --------------\n",
            "Choose move number: 3\n",
            "  --------------\n",
            "[['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '-' '0' '0' '0']\n",
            " ['0' '0' '0' '*' '0' '0' '0']\n",
            " ['0' '0' '-' '*' '0' '0' '0']\n",
            " ['0' '-' '-' '*' '*' '*' '0']]\n",
            "  --------------\n",
            "Choose move number: 7\n",
            "  --------------\n",
            "[['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '-' '0' '0' '0']\n",
            " ['0' '0' '0' '*' '0' '0' '0']\n",
            " ['0' '0' '-' '*' '0' '*' '0']\n",
            " ['0' '-' '-' '*' '*' '*' '-']]\n",
            "  --------------\n",
            "Choose move number: 3\n",
            "  --------------\n",
            "[['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '*' '-' '0' '0' '0']\n",
            " ['0' '0' '-' '*' '0' '0' '0']\n",
            " ['0' '0' '-' '*' '0' '*' '0']\n",
            " ['0' '-' '-' '*' '*' '*' '-']]\n",
            "  --------------\n",
            "Choose move number: 2\n",
            "  --------------\n",
            "[['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '*' '-' '0' '0' '0']\n",
            " ['0' '*' '-' '*' '0' '0' '0']\n",
            " ['0' '-' '-' '*' '0' '*' '0']\n",
            " ['0' '-' '-' '*' '*' '*' '-']]\n",
            "  --------------\n",
            "Choose move number: 1\n",
            "winner is: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtoE8FqWblt1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}